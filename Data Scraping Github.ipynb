{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **COMMENT SCRAPING FROM REDDIT USING REDDIT API**\n",
        "\n",
        "In this project, we'll use the Python ***Reddit API Wrapper (PRAW)*** to fetch comments from a specific subreddit and save them into CSV files for further analysis. Let's dive into the code and understand how each part contributes to the project."
      ],
      "metadata": {
        "id": "fSuagytAQSAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**STEP 1: Installing Necessary Libraries**\n",
        "\n",
        "First, we import all the required libraries. These include ***praw*** for accessing Reddit, ***datetime*** for handling date and time, ***requests*** for making HTTP requests, ***csv*** for writing data to CSV files, and ***os*** for handling file directories."
      ],
      "metadata": {
        "id": "b4x1hwpzQ5Z5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWVS1oG2IIbk",
        "outputId": "436355c9-4e16-42e8-a6ae-bf84c922532a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n",
        "import datetime\n",
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "from os.path import join\n",
        "import praw\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 2: Setting Up Reddit API Credentials**\n",
        "\n",
        "We set up the Reddit API credentials using PRAW.\n"
      ],
      "metadata": {
        "id": "Fmo1sJ64RU65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = praw.Reddit(client_id='api_client_id',\n",
        "                     client_secret='client_secret',\n",
        "                     user_agent='Aditya Bhandari',\n",
        "                     check_for_async=False)\n",
        "data_directory = \"data\"\n",
        "os.makedirs(data_directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "PjCU2vBxNCfY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 3: Defining Variables**\n",
        "Here, we define the subreddit we are interested in and initialize an empty list to store the fetched comments."
      ],
      "metadata": {
        "id": "QV180qA1Ru4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subreddit from which we will get comments\n",
        "\n",
        "subreddit_name = 'OpenAI'\n",
        "\n",
        "# List to store comments\n",
        "comments_data = []\n",
        "\n",
        "filepath = \"D:\\\\Spring 24 Study\\\\Research and Communication\\\\Research Paper  - Aditya and Reina\\\\Data and code\"\n"
      ],
      "metadata": {
        "id": "nPSDzi9FNaNA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 4: Fetching Comments**\n",
        "We define a ***function fetch_comments_per_month*** that fetches comments from the specified subreddit for a given month and year.\n",
        "\n",
        "The function uses ***PRAW*** to interact with Reddit and filter comments based on specific keywords."
      ],
      "metadata": {
        "id": "gp1GJJVcSaXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_comments_per_month(subreddit_name, year, month, limit=1000):\n",
        "    comments_data = []\n",
        "    start_time = datetime.datetime(year, month, 1)\n",
        "    if month == 12:\n",
        "        end_time = datetime.datetime(year + 1, 1, 1)\n",
        "    else:\n",
        "        end_time = datetime.datetime(year, month + 1, 1)\n",
        "\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "    # comments containing these keywords will be fetched\n",
        "    keywords = ['chatgpt', 'gpt', 'ai', 'LLM']\n",
        "\n",
        "    for submission in subreddit.new(limit=None):\n",
        "        submission_time = datetime.datetime.fromtimestamp(submission.created_utc)\n",
        "        if start_time <= submission_time < end_time:\n",
        "            submission.comments.replace_more(limit=0)\n",
        "            for comment in submission.comments.list():\n",
        "                if len(comments_data) < limit:\n",
        "                    comment_text = comment.body.lower()\n",
        "                    if any(keyword in comment_text for keyword in keywords):\n",
        "                        comments_data.append({\n",
        "                            'body': comment.body,\n",
        "                            'created_utc': datetime.datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                            'permalink': f\"https://reddit.com{comment.permalink}\"\n",
        "                        })\n",
        "                else:\n",
        "                    return comments_data\n",
        "    return comments_data\n"
      ],
      "metadata": {
        "id": "VCGgQIFANbu1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 5: Saving Comments to CSV**\n",
        "We define a function ***save_comments_to_csv*** that saves the fetched comments into a CSV file.\n",
        "\n",
        "We have new csv file for each month."
      ],
      "metadata": {
        "id": "WL8ZnAw8SqGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_comments_to_csv(comments_data, year, month):\n",
        "    data_directory = \"data\"\n",
        "    os.makedirs(data_directory, exist_ok=True)\n",
        "    csv_file_name = f\"comments_{year}_{month:02d}.csv\"\n",
        "    csv_file_path = os.path.join(data_directory, csv_file_name)\n",
        "\n",
        "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "        fieldnames = ['body', 'created_utc', 'permalink']\n",
        "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for comment in comments_data:\n",
        "            writer.writerow(comment)\n",
        "\n",
        "    print(f\"Data saved to {filepath} with {len(comments_data)} comments.\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "vVS68QGNNv2M"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 6: Running the Data Collection**\n",
        "We set the start and end dates and run a loop to fetch and save comments month by month. This ensures that we collect data for the entire period from the start date to the current date."
      ],
      "metadata": {
        "id": "yqmQk2d9TtW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = datetime.datetime(2024, 1, 1)\n",
        "end_date = datetime.datetime.now()\n",
        "\n",
        "current_date = start_date\n",
        "while current_date < end_date:\n",
        "    year = current_date.year\n",
        "    month = current_date.month\n",
        "    comments_data = fetch_comments_per_month(subreddit_name, year, month)\n",
        "    save_comments_to_csv(comments_data, year, month)\n",
        "    if month == 12:\n",
        "        current_date = datetime.datetime(year + 1, 1, 1)\n",
        "    else:\n",
        "        current_date = datetime.datetime(year, month + 1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2gYfwLJNzAY",
        "outputId": "a90e217a-7af0-40f2-bb43-8a2a3f61183c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to D:\\Spring 24 Study\\Research and Communication\\Research Paper  - Aditya and Reina\\Data and code with 0 comments.\n",
            "Data saved to D:\\Spring 24 Study\\Research and Communication\\Research Paper  - Aditya and Reina\\Data and code with 0 comments.\n",
            "Data saved to D:\\Spring 24 Study\\Research and Communication\\Research Paper  - Aditya and Reina\\Data and code with 0 comments.\n",
            "Data saved to D:\\Spring 24 Study\\Research and Communication\\Research Paper  - Aditya and Reina\\Data and code with 0 comments.\n",
            "Data saved to D:\\Spring 24 Study\\Research and Communication\\Research Paper  - Aditya and Reina\\Data and code with 1000 comments.\n",
            "Data saved to D:\\Spring 24 Study\\Research and Communication\\Research Paper  - Aditya and Reina\\Data and code with 1000 comments.\n",
            "Data saved to D:\\Spring 24 Study\\Research and Communication\\Research Paper  - Aditya and Reina\\Data and code with 1000 comments.\n"
          ]
        }
      ]
    }
  ]
}